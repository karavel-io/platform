{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Karavel? \u00b6 Karavel is a project that provides tools and knowledge around the Kubernetes stack to deploy and manage private Containers-as-a-Service platforms in the Cloud and on premise. The main output of the Karavel project is the Karavel Container Platform , a curated set of components and services based on the best-in-breed open source projects, carefully configured to deliver a production-ready platform for enterprises. The Karavel Container Platform \u00b6 Karavel provides many different components that together form a cohesive and integrated environment called the Karavel Container Platform . The Karavel Container Platform selects specific versions of these components that are tried and tested together and publishes them as a ready-to-use kit to assemble GitOps-enabled platforms for enterprises. Check out our quickstart guide for an introductory view to the Platform. Features \u00b6 Run on any conformant Kubernetes cluster 100% open source stack based on community and CNCF projects GitOps first workflow, enabling the platform to be self-hosted and updating itself Built-in security tools for secrets management, policy enforcement and access control Elastic routing layer with automated DNS, load balancing and certificate management Comprehensive observability stack with metrics, logging and distributed tracing collection and visualization","title":"Overview"},{"location":"#what-is-karavel","text":"Karavel is a project that provides tools and knowledge around the Kubernetes stack to deploy and manage private Containers-as-a-Service platforms in the Cloud and on premise. The main output of the Karavel project is the Karavel Container Platform , a curated set of components and services based on the best-in-breed open source projects, carefully configured to deliver a production-ready platform for enterprises.","title":"What is Karavel?"},{"location":"#the-karavel-container-platform","text":"Karavel provides many different components that together form a cohesive and integrated environment called the Karavel Container Platform . The Karavel Container Platform selects specific versions of these components that are tried and tested together and publishes them as a ready-to-use kit to assemble GitOps-enabled platforms for enterprises. Check out our quickstart guide for an introductory view to the Platform.","title":"The Karavel Container Platform"},{"location":"#features","text":"Run on any conformant Kubernetes cluster 100% open source stack based on community and CNCF projects GitOps first workflow, enabling the platform to be self-hosted and updating itself Built-in security tools for secrets management, policy enforcement and access control Elastic routing layer with automated DNS, load balancing and certificate management Comprehensive observability stack with metrics, logging and distributed tracing collection and visualization","title":"Features"},{"location":"cli/","text":"Karavel CLI \u00b6 The Karavel CLI tool is a utility that helps to manage GitOps repositories for installing Karavel on Kubernetes clusters. Install \u00b6 Binaries for all mainstream operating systems can be downloaded from GitHub . Nix \u00b6 The CLI is packaged as a Flake . You can run it as a simple command: nix run github:karavel-io/cli <regular karavel arguments> e.g. nix run github:karavel-io/cli render --debug Or you can import it in another flake.nix , e.g. to add it to a devShell : { inputs = { nixpkgs . url = \"github:nixos/nixpkgs\" ; flake-utils . url = \"github:numtide/flake-utils\" ; karavel . url = \"github:karavel-io/cli\" ; }; outputs = { self , nixpkgs , flake-utils , karavel }: flake-utils . lib . eachDefaultSystem ( system : let pkgs = nixpkgs . legacyPackages . $ { system }; karavel-cli = karavel . defaultPackage . $ { system }; in { devShell = pkgs . mkShell { buildInputs = with pkgs ; [ karavel-cli kubectl kustomize ]; }; }); } By appending a refernce at the end of the Flake URL you can select: a specific tag/version: github:karavel-io/cli/v0.4.1 a Git branch: github:karavel-io/cli/main More information on references can be found on the Flakes manual . Docker \u00b6 The CLI is packaged in a container image and published on Quay and GitHub . You can run it like so: # Inside a Karavel project directory $ docker run --rm -v $PWD :/karavel -u ( id -u ) quay.io/karavel/cli:main render $ docker run --rm -v $PWD :/karavel -u ( id -u ) ghcr.io/karavel-io/cli:main render Stable releases are tagged using their semver ( x.y.z ) version, with aliases to the latest patch ( x.y ) and minor ( x ) versions. This is what you should be using most of the time. The main tag points to the latest unstable build from the main branch. It's useful if you want to try out the latest features before they are released. From sources \u00b6 Instructions for compiling the tool from source code can be found on GitHub . karavel init \u00b6 Initialize a new Karavel project Usage: karavel init [WORKDIR] [flags] Flags: --force Overwrite the config file even if it already exists --github-repo string Override the official GitHub repository containing the tagged Platform releases -h, --help help for init -o, --output-file string Karavel config file name to create (default \"karavel.hcl\") -v, --version string Karavel Container Platform version to initialize (default \"latest\") Global Flags: --colors Enable colored logs (default true) -d, --debug Output debug logs -q, --quiet Suppress all logs except errors The Karavel team regularly publishes recommended selections of components with matching versions that can be used as a starting point for your clusters. These starting configurations are completely optional and can be tweaked to accomodate your specific setup, or you could write your own from scratch. However, they are a useful starting point as the provided component versions combinations have been carefully tested to ensure they are fully compatible with each other. This command will download the recommended base config for the latest version of Karavel to the current directory: $ cd /tmp/karavel $ karavel init Initializing new Karavel latest project at /tmp/karavel Fetching latest release version for GitHub repo karavel-io/platform Writing config file to /tmp/karavel/karavel.hcl To download a specific version of the Karavel Container Platform instead of the latest one, you can pass the --version flag to karavel init with the desired version. $ cd /tmp/karavel $ karavel init --version 2021 .1 Initializing new Karavel 2021 .1 project at /tmp/karavel Writing config file to /tmp/karavel/karavel.hcl You can now safely edit the karavel.hcl file to configure the platform based on your environment. There are a few parameters that some component need in order to properly function, like OIDC and Cloud providers credentials. More information is provided in the next pages. karavel render \u00b6 Render a Karavel project with the given config (defaults to 'karavel.hcl' in the current directory). This command is idempotent and can be run multiple times without issues. It will respect changes made to files outside the 'vendor' directory, only adding or removing Karavel-specific entries. It will, however, consider the 'vendor' directory as a fully-managed folder and may add, delete or modify any file inside it without warning. Usage: karavel render [flags] Flags: -f, --file string Specify an alternate config file (default \"karavel.hcl\") -h, --help help for render --skip-git Skip the git integration to discover remote repositories for Argo. WARNING: this will render the Argo component inoperable Global Flags: --colors Enable colored logs (default true) -d, --debug Output debug logs -q, --quiet Suppress all logs except errors karavel render is the primary tool used to manage Karavel GitOps repositories. It takes a HCL configuration file that describes the required components, their version and their parameters, and generates the appropriate Kubernetes manifests for them. These manifests will install all the required components in one go from a set of curated packages maintained by the Karavel project. The command will also enable or disable bits of configuration based on the available components, enabling useful integrations without the user having to worry about wiring all the pieces together on its own. One common example would be adding ServiceMonitor definitions to all the components that provide Prometheus metrics if the prometheus component is added to the configuration, so that metrics can be scraped and visualized in Grafana. Once bootstrapped a new cluster is completely self-managed, with ArgoCD acting as the GitOps engine in charge of maintaining the deployed services in sync with their manifests stored in your Git provider of choice. Changing a configuration and redeploying becomes as easy as committing and pushing to a Git repository. Check the Quickstart Guide to see this tool in action.","title":"CLI"},{"location":"cli/#karavel-cli","text":"The Karavel CLI tool is a utility that helps to manage GitOps repositories for installing Karavel on Kubernetes clusters.","title":"Karavel CLI"},{"location":"cli/#install","text":"Binaries for all mainstream operating systems can be downloaded from GitHub .","title":"Install"},{"location":"cli/#nix","text":"The CLI is packaged as a Flake . You can run it as a simple command: nix run github:karavel-io/cli <regular karavel arguments> e.g. nix run github:karavel-io/cli render --debug Or you can import it in another flake.nix , e.g. to add it to a devShell : { inputs = { nixpkgs . url = \"github:nixos/nixpkgs\" ; flake-utils . url = \"github:numtide/flake-utils\" ; karavel . url = \"github:karavel-io/cli\" ; }; outputs = { self , nixpkgs , flake-utils , karavel }: flake-utils . lib . eachDefaultSystem ( system : let pkgs = nixpkgs . legacyPackages . $ { system }; karavel-cli = karavel . defaultPackage . $ { system }; in { devShell = pkgs . mkShell { buildInputs = with pkgs ; [ karavel-cli kubectl kustomize ]; }; }); } By appending a refernce at the end of the Flake URL you can select: a specific tag/version: github:karavel-io/cli/v0.4.1 a Git branch: github:karavel-io/cli/main More information on references can be found on the Flakes manual .","title":"Nix"},{"location":"cli/#docker","text":"The CLI is packaged in a container image and published on Quay and GitHub . You can run it like so: # Inside a Karavel project directory $ docker run --rm -v $PWD :/karavel -u ( id -u ) quay.io/karavel/cli:main render $ docker run --rm -v $PWD :/karavel -u ( id -u ) ghcr.io/karavel-io/cli:main render Stable releases are tagged using their semver ( x.y.z ) version, with aliases to the latest patch ( x.y ) and minor ( x ) versions. This is what you should be using most of the time. The main tag points to the latest unstable build from the main branch. It's useful if you want to try out the latest features before they are released.","title":"Docker"},{"location":"cli/#from-sources","text":"Instructions for compiling the tool from source code can be found on GitHub .","title":"From sources"},{"location":"cli/#karavel-init","text":"Initialize a new Karavel project Usage: karavel init [WORKDIR] [flags] Flags: --force Overwrite the config file even if it already exists --github-repo string Override the official GitHub repository containing the tagged Platform releases -h, --help help for init -o, --output-file string Karavel config file name to create (default \"karavel.hcl\") -v, --version string Karavel Container Platform version to initialize (default \"latest\") Global Flags: --colors Enable colored logs (default true) -d, --debug Output debug logs -q, --quiet Suppress all logs except errors The Karavel team regularly publishes recommended selections of components with matching versions that can be used as a starting point for your clusters. These starting configurations are completely optional and can be tweaked to accomodate your specific setup, or you could write your own from scratch. However, they are a useful starting point as the provided component versions combinations have been carefully tested to ensure they are fully compatible with each other. This command will download the recommended base config for the latest version of Karavel to the current directory: $ cd /tmp/karavel $ karavel init Initializing new Karavel latest project at /tmp/karavel Fetching latest release version for GitHub repo karavel-io/platform Writing config file to /tmp/karavel/karavel.hcl To download a specific version of the Karavel Container Platform instead of the latest one, you can pass the --version flag to karavel init with the desired version. $ cd /tmp/karavel $ karavel init --version 2021 .1 Initializing new Karavel 2021 .1 project at /tmp/karavel Writing config file to /tmp/karavel/karavel.hcl You can now safely edit the karavel.hcl file to configure the platform based on your environment. There are a few parameters that some component need in order to properly function, like OIDC and Cloud providers credentials. More information is provided in the next pages.","title":"karavel init"},{"location":"cli/#karavel-render","text":"Render a Karavel project with the given config (defaults to 'karavel.hcl' in the current directory). This command is idempotent and can be run multiple times without issues. It will respect changes made to files outside the 'vendor' directory, only adding or removing Karavel-specific entries. It will, however, consider the 'vendor' directory as a fully-managed folder and may add, delete or modify any file inside it without warning. Usage: karavel render [flags] Flags: -f, --file string Specify an alternate config file (default \"karavel.hcl\") -h, --help help for render --skip-git Skip the git integration to discover remote repositories for Argo. WARNING: this will render the Argo component inoperable Global Flags: --colors Enable colored logs (default true) -d, --debug Output debug logs -q, --quiet Suppress all logs except errors karavel render is the primary tool used to manage Karavel GitOps repositories. It takes a HCL configuration file that describes the required components, their version and their parameters, and generates the appropriate Kubernetes manifests for them. These manifests will install all the required components in one go from a set of curated packages maintained by the Karavel project. The command will also enable or disable bits of configuration based on the available components, enabling useful integrations without the user having to worry about wiring all the pieces together on its own. One common example would be adding ServiceMonitor definitions to all the components that provide Prometheus metrics if the prometheus component is added to the configuration, so that metrics can be scraped and visualized in Grafana. Once bootstrapped a new cluster is completely self-managed, with ArgoCD acting as the GitOps engine in charge of maintaining the deployed services in sync with their manifests stored in your Git provider of choice. Changing a configuration and redeploying becomes as easy as committing and pushing to a Git repository. Check the Quickstart Guide to see this tool in action.","title":"karavel render"},{"location":"components/","text":"Overview \u00b6 The Karavel Container Platform is built as a modular collection of components that interlock with one another to form a cohesive GitOps platform. This section contains documentation for each available component. Please note that the documentation source code is stored in each specific component's repository. Changes to these pages should be made in the upstream repository and not in the central platform repo. ArgoCD cert Dex External External Goldpinger Ingress Grafana Loki Olm Prometheus Tempo Velero","title":"Overview"},{"location":"components/#overview","text":"The Karavel Container Platform is built as a modular collection of components that interlock with one another to form a cohesive GitOps platform. This section contains documentation for each available component. Please note that the documentation source code is stored in each specific component's repository. Changes to these pages should be made in the upstream repository and not in the central platform repo. ArgoCD cert Dex External External Goldpinger Ingress Grafana Loki Olm Prometheus Tempo Velero","title":"Overview"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Karavel Container Platform \u00b6 Q: Is the Karavel Container Platform a certified Kubernetes distribution? \u00b6 A: No, we do not bundle Kubernetes nor provide any means of installing it as part of the platform offering. The Karavel Container Platform expects a conformant Kubernetes cluster with enough spare capacity to be present before deploying. We only provide software components and addons that are deployed on top of the cluster to build up a comprehensive and production-grade environment for developers to run and operate their workloads. The Karavel Container Platform is virtually compatible with any Kubernetes cluster, although there may be some cases where existing addons may conflict with the Karavel components (for example, Red Hat OpenShift already provides the Prometheus Operator as part of its core). A bare, upstream Kubernetes without any extra component (not even a CNI) is the ideal starting point for deploying the Platform. Q: Are components available as Helm charts? \u00b6 A: Well yes, but actually no. While the platform components are indeed packaged as Helm charts , they are not meant to be installed with Helm (e.g. via helm install ). Instead, they are consumed by the Karavel CLI , templated based on the provided Karavel configuration file, and then wrote to disk in a specific directory structure (documented here ) that will be set-up as Kustomize stacks. These Kustomize stacks are then deployed by ArgoCD onto the target cluster. While you could go ahead and install the components via Helm directly, they are not designed for this usage and have way less configuration parameters than the upstream chart they are based on. They provide highly opinionated configurations that are meant to work in concert with the rest of the Karavel stack and any customization is handled through Kustomize patches. If you want to install, say, Prometheus with Helm, you should use the official chart instead. Q: Can I swap component X for alternative Y? \u00b6 A: The Karavel Container Platform builds on a set of principles and assumptions based on the team's experience running Kubernetes in production. The available components have been selected and their configuration carefully crafted to integrate with each other and compose a robust production-grade platform with a great developer experience out of the box, so while a few of them have alternative implementations that can be swapped in and out to accomodate specific scenarios, it is unlikely that we'll provide other solutions for the core components. If you have a specific need that is not satisfied by the platform current state, please reach out to the maintainers , we'll be happy to help you!","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#karavel-container-platform","text":"","title":"Karavel Container Platform"},{"location":"faq/#q-is-the-karavel-container-platform-a-certified-kubernetes-distribution","text":"A: No, we do not bundle Kubernetes nor provide any means of installing it as part of the platform offering. The Karavel Container Platform expects a conformant Kubernetes cluster with enough spare capacity to be present before deploying. We only provide software components and addons that are deployed on top of the cluster to build up a comprehensive and production-grade environment for developers to run and operate their workloads. The Karavel Container Platform is virtually compatible with any Kubernetes cluster, although there may be some cases where existing addons may conflict with the Karavel components (for example, Red Hat OpenShift already provides the Prometheus Operator as part of its core). A bare, upstream Kubernetes without any extra component (not even a CNI) is the ideal starting point for deploying the Platform.","title":"Q: Is the Karavel Container Platform a certified Kubernetes distribution?"},{"location":"faq/#q-are-components-available-as-helm-charts","text":"A: Well yes, but actually no. While the platform components are indeed packaged as Helm charts , they are not meant to be installed with Helm (e.g. via helm install ). Instead, they are consumed by the Karavel CLI , templated based on the provided Karavel configuration file, and then wrote to disk in a specific directory structure (documented here ) that will be set-up as Kustomize stacks. These Kustomize stacks are then deployed by ArgoCD onto the target cluster. While you could go ahead and install the components via Helm directly, they are not designed for this usage and have way less configuration parameters than the upstream chart they are based on. They provide highly opinionated configurations that are meant to work in concert with the rest of the Karavel stack and any customization is handled through Kustomize patches. If you want to install, say, Prometheus with Helm, you should use the official chart instead.","title":"Q: Are components available as Helm charts?"},{"location":"faq/#q-can-i-swap-component-x-for-alternative-y","text":"A: The Karavel Container Platform builds on a set of principles and assumptions based on the team's experience running Kubernetes in production. The available components have been selected and their configuration carefully crafted to integrate with each other and compose a robust production-grade platform with a great developer experience out of the box, so while a few of them have alternative implementations that can be swapped in and out to accomodate specific scenarios, it is unlikely that we'll provide other solutions for the core components. If you have a specific need that is not satisfied by the platform current state, please reach out to the maintainers , we'll be happy to help you!","title":"Q: Can I swap component X for alternative Y?"},{"location":"quickstart/","text":"Quickstart \u00b6 This section will guide you through the first bootstrap of a new Karavel cluster from scratch. It assumes you have a general knowledge of Git and Kubernetes and know your way around a command shell. Requirements \u00b6 Tools \u00b6 the karavel CLI tool git , kubectl , kustomize installed on your local machine admin access to a running Kubernetes cluster. Minikube is fine for a simple, local deployment for testing purposes. The karavel CLI can be downloaded from GitHub . Alternatively, you can build it from source. You need Golang 1.18+ installed to build it. go install github.com/karavel-io/cli/cmd/karavel@latest More instructions for installing the CLI can be found here . Secrets \u00b6 Karavel delegates the handling of secrets and credentials to an external service. This approach has been chosen in order to avoid having secrets stored in plain text inside Git repository or needing complex encryption mechanisms to protect them. Karavel leverages the External Secrets controller to fetch credentials from the backing service and convert them into regular Kubernetes Secrets objects to be consumed by pods. This way no changes to the application or special tools are required. Credentials needed by core Karavel components are provisioned using this same method. This includes Git provider credentials for ArgoCD, DNS providers tokens for managing DNS records and OpenID Connect client secrets for authentication. The exact secrets required by each component are referenced in their specific page in the Components section . They need to be provisioned beforehand and External Secrets needs the appropriate permissions to access them, otherwise the platform will not be able to run. They can be managed using whatever method is preferred, being it manually or via Infrastructure as Code tools like Terraform or CDK. Repository setup \u00b6 Karavel builds on the principles of GitOps, whereby the entire state of the system is written to git repositories and periodically synced with the live cluster. Let's prepare a new repository. $ mkdir my-karavel-infra && cd my-karavel-infra $ git init Initialized empty Git repository in /home/user/my-karavel-infra/.git/ Now we are ready to generate the base Karavel manifests for deployment. Bootstrap \u00b6 Each Karavel component is packaged and distributed as a Helm chart . The Karavel CLI tool then uses these charts to generate the final YAML files that will be installed on the cluster. Ideally, as an end user you will never need to interact with the charts directly. Here is a super quick demo of the bootstrap process. The CLI tool uses a simple HCL file to describe the required components and their configuration. Components will also automatically enable or disable some optional features based on what other components are available. For example, a component may enable metrics collection if prometheus is present as well. A component definition consists of a named block featuring the required version, target namespace and configuration params that will be used to render out the manifests. component \"name\" { version = \"major.minor.patch\" namespace = \"target-namespace\" # Params param1 = true param2 = { example = \"value\" } } Here is an example with the cert-manager component: component \"cert-manager\" { version = \"0.1.0\" namespace = \"cert-manager\" letsencrypt = { email = \"tech@karavel.io\" } } Write your desired selection of components to a file called karavel.hcl . The karavel render command will use this file to assemble a plan and render the proper configuration. Info Instead of writing the karavel.hcl file from scratch, it is recommended to run karavel init to fetch the base config from the official repository. The Karavel team regularly publishes recommended selections of components with matching versions that can be used as a starting point for your clusters. To download a specific version of the Karavel Container Platform instead of the latest one, you can pass the --version flag to karavel init with the desired version. See the CLI reference for more information. When you're satisfied with your configuration, simply running karavel render in the same directory as the file will download the required components from the repository and generate the appropriate directory structure. $ karavel render Rendering new Karavel project with config file /tmp/karavel/karavel.hcl Rendering component 'loki' 0 .1.0-SNAPSHOT at karavel/vendor/loki Rendering component 'velero' 0 .1.0-SNAPSHOT at karavel/vendor/velero Rendering component 'goldpinger' 0 .1.0-SNAPSHOT at karavel/vendor/goldpinger Rendering component 'argocd' 0 .1.0-SNAPSHOT at karavel/vendor/argocd Rendering component 'cert-manager' 0 .1.0-SNAPSHOT at karavel/vendor/cert-manager Rendering component 'dex' 0 .1.0-SNAPSHOT at karavel/vendor/dex Rendering component 'external-secrets' 0 .1.0-SNAPSHOT at karavel/vendor/external-secrets Rendering component 'external-dns' 0 .1.0-SNAPSHOT at karavel/vendor/external-dns Rendering component 'prometheus' 0 .1.0-SNAPSHOT at karavel/vendor/prometheus Rendering component 'grafana' 0 .1.0-SNAPSHOT at karavel/vendor/grafana Rendering component 'tempo' 0 .1.0-SNAPSHOT at karavel/vendor/tempo Rendering component 'olm' 0 .1.0-SNAPSHOT at karavel/vendor/olm Rendering component 'ingress-nginx' 0 .1.0-SNAPSHOT at karavel/vendor/ingress-nginx Here's a view of the generated folders. . \u251c\u2500\u2500 applications \u2502 \u251c\u2500\u2500 argocd.yml \u2502 \u251c\u2500\u2500 cert-manager.yml \u2502 \u251c\u2500\u2500 dex.yml \u2502 \u251c\u2500\u2500 external-dns.yml \u2502 \u251c\u2500\u2500 external-secrets.yml \u2502 \u251c\u2500\u2500 goldpinger.yml \u2502 \u251c\u2500\u2500 grafana.yml \u2502 \u251c\u2500\u2500 ingress-nginx.yml \u2502 \u251c\u2500\u2500 kustomization.yml \u2502 \u251c\u2500\u2500 loki.yml \u2502 \u251c\u2500\u2500 olm.yml \u2502 \u251c\u2500\u2500 prometheus.yml \u2502 \u251c\u2500\u2500 tempo.yml \u2502 \u2514\u2500\u2500 velero.yml \u251c\u2500\u2500 karavel.hcl \u251c\u2500\u2500 kustomization.yml \u251c\u2500\u2500 projects \u2502 \u251c\u2500\u2500 infrastructure.yml \u2502 \u2514\u2500\u2500 kustomization.yml \u2514\u2500\u2500 vendor \u251c\u2500\u2500 argocd \u251c\u2500\u2500 cert-manager \u251c\u2500\u2500 dex \u251c\u2500\u2500 external-dns \u251c\u2500\u2500 external-secrets \u251c\u2500\u2500 goldpinger \u251c\u2500\u2500 grafana \u251c\u2500\u2500 ingress-nginx \u251c\u2500\u2500 loki \u251c\u2500\u2500 olm \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 tempo \u2514\u2500\u2500 velero The vendor folder contains Kubernetes manifests for all Karavel components. Files in this folder are not supposed to be edited by hand, as any change would be overwritten by future Karavel updates. If you need to customize them follow the Customization guide . The applications directory contains ArgoCD application manifests that are used to register the vendored components with the GitOps engine. These files can be safely edited to customize how ArgoCD manages them. The projects directory contains ArgoCD project manifests. It will only contain one project, infrastructure , which is used to group all installed Karavel applications. Feel free to add your own. Finally, kustomization.yml is a Kustomize stack that references all the vendored components that are needed to bootstrap the cluster, as well as the application manifests, and can be used to install the entire platform in one go. Once the bootstrap components are up and running, ArgoCD will pick the rest of the applications up and take care of deploying the rest of the stack. To bootstrap the cluster, run the following command while connected to it: kustomize build . | kubectl apply -f - Warning Due to the way kubectl applies multiple manifests at once, there may be some race conditions between created resources. If you get some errors along the lines of no matches for kind \"AppProject\" in version \"argoproj.io/v1alpha1\" just rerun the command again. Also notice that some custom resources provided by non-bootstrap components (e.g. ServiceMonitor provided by prometheus ) will fail to install because their component is not part of the bootstrap process. This is fine and can be ignored. Once ArgoCD is up and running it will take care of updating the missing parts. You can check that all bootstrap components are correctly deployed by running kubectl get pods --all-namespaces . Git Push \u00b6 Now that the repository is ready we can commit and push it to the upstream remote. git add --all . && git commit -m \"Bootstrap new cluster\" git push origin main Once pushed ArgoCD will automatically pick it up and start syncing the manifests with the cluster state. New changes can be deployed by simply committing them and pushing to origin . Congratulations, you are now running a full fledged Karavel instance! Next steps \u00b6 Updating components \u00b6 Karavel publishes regular updates to its components, either to fix or improve their manifests or to introduce new features and integrations between them. Updates are also released when a new Kubernetes version is published, to keep components compatible (see the Karavel compatibility matrix ). You should really strive to maintain your clusters up to date with the latest Kubernetes release, and Karavel should be updated consequently. To do so, simply change the required component versions and update the parameters if necessary based on the updated documentation, then rerun the karavel render command. This command is idempotent, so it is safe to run multiple times.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This section will guide you through the first bootstrap of a new Karavel cluster from scratch. It assumes you have a general knowledge of Git and Kubernetes and know your way around a command shell.","title":"Quickstart"},{"location":"quickstart/#requirements","text":"","title":"Requirements"},{"location":"quickstart/#tools","text":"the karavel CLI tool git , kubectl , kustomize installed on your local machine admin access to a running Kubernetes cluster. Minikube is fine for a simple, local deployment for testing purposes. The karavel CLI can be downloaded from GitHub . Alternatively, you can build it from source. You need Golang 1.18+ installed to build it. go install github.com/karavel-io/cli/cmd/karavel@latest More instructions for installing the CLI can be found here .","title":"Tools"},{"location":"quickstart/#secrets","text":"Karavel delegates the handling of secrets and credentials to an external service. This approach has been chosen in order to avoid having secrets stored in plain text inside Git repository or needing complex encryption mechanisms to protect them. Karavel leverages the External Secrets controller to fetch credentials from the backing service and convert them into regular Kubernetes Secrets objects to be consumed by pods. This way no changes to the application or special tools are required. Credentials needed by core Karavel components are provisioned using this same method. This includes Git provider credentials for ArgoCD, DNS providers tokens for managing DNS records and OpenID Connect client secrets for authentication. The exact secrets required by each component are referenced in their specific page in the Components section . They need to be provisioned beforehand and External Secrets needs the appropriate permissions to access them, otherwise the platform will not be able to run. They can be managed using whatever method is preferred, being it manually or via Infrastructure as Code tools like Terraform or CDK.","title":"Secrets"},{"location":"quickstart/#repository-setup","text":"Karavel builds on the principles of GitOps, whereby the entire state of the system is written to git repositories and periodically synced with the live cluster. Let's prepare a new repository. $ mkdir my-karavel-infra && cd my-karavel-infra $ git init Initialized empty Git repository in /home/user/my-karavel-infra/.git/ Now we are ready to generate the base Karavel manifests for deployment.","title":"Repository setup"},{"location":"quickstart/#bootstrap","text":"Each Karavel component is packaged and distributed as a Helm chart . The Karavel CLI tool then uses these charts to generate the final YAML files that will be installed on the cluster. Ideally, as an end user you will never need to interact with the charts directly. Here is a super quick demo of the bootstrap process. The CLI tool uses a simple HCL file to describe the required components and their configuration. Components will also automatically enable or disable some optional features based on what other components are available. For example, a component may enable metrics collection if prometheus is present as well. A component definition consists of a named block featuring the required version, target namespace and configuration params that will be used to render out the manifests. component \"name\" { version = \"major.minor.patch\" namespace = \"target-namespace\" # Params param1 = true param2 = { example = \"value\" } } Here is an example with the cert-manager component: component \"cert-manager\" { version = \"0.1.0\" namespace = \"cert-manager\" letsencrypt = { email = \"tech@karavel.io\" } } Write your desired selection of components to a file called karavel.hcl . The karavel render command will use this file to assemble a plan and render the proper configuration. Info Instead of writing the karavel.hcl file from scratch, it is recommended to run karavel init to fetch the base config from the official repository. The Karavel team regularly publishes recommended selections of components with matching versions that can be used as a starting point for your clusters. To download a specific version of the Karavel Container Platform instead of the latest one, you can pass the --version flag to karavel init with the desired version. See the CLI reference for more information. When you're satisfied with your configuration, simply running karavel render in the same directory as the file will download the required components from the repository and generate the appropriate directory structure. $ karavel render Rendering new Karavel project with config file /tmp/karavel/karavel.hcl Rendering component 'loki' 0 .1.0-SNAPSHOT at karavel/vendor/loki Rendering component 'velero' 0 .1.0-SNAPSHOT at karavel/vendor/velero Rendering component 'goldpinger' 0 .1.0-SNAPSHOT at karavel/vendor/goldpinger Rendering component 'argocd' 0 .1.0-SNAPSHOT at karavel/vendor/argocd Rendering component 'cert-manager' 0 .1.0-SNAPSHOT at karavel/vendor/cert-manager Rendering component 'dex' 0 .1.0-SNAPSHOT at karavel/vendor/dex Rendering component 'external-secrets' 0 .1.0-SNAPSHOT at karavel/vendor/external-secrets Rendering component 'external-dns' 0 .1.0-SNAPSHOT at karavel/vendor/external-dns Rendering component 'prometheus' 0 .1.0-SNAPSHOT at karavel/vendor/prometheus Rendering component 'grafana' 0 .1.0-SNAPSHOT at karavel/vendor/grafana Rendering component 'tempo' 0 .1.0-SNAPSHOT at karavel/vendor/tempo Rendering component 'olm' 0 .1.0-SNAPSHOT at karavel/vendor/olm Rendering component 'ingress-nginx' 0 .1.0-SNAPSHOT at karavel/vendor/ingress-nginx Here's a view of the generated folders. . \u251c\u2500\u2500 applications \u2502 \u251c\u2500\u2500 argocd.yml \u2502 \u251c\u2500\u2500 cert-manager.yml \u2502 \u251c\u2500\u2500 dex.yml \u2502 \u251c\u2500\u2500 external-dns.yml \u2502 \u251c\u2500\u2500 external-secrets.yml \u2502 \u251c\u2500\u2500 goldpinger.yml \u2502 \u251c\u2500\u2500 grafana.yml \u2502 \u251c\u2500\u2500 ingress-nginx.yml \u2502 \u251c\u2500\u2500 kustomization.yml \u2502 \u251c\u2500\u2500 loki.yml \u2502 \u251c\u2500\u2500 olm.yml \u2502 \u251c\u2500\u2500 prometheus.yml \u2502 \u251c\u2500\u2500 tempo.yml \u2502 \u2514\u2500\u2500 velero.yml \u251c\u2500\u2500 karavel.hcl \u251c\u2500\u2500 kustomization.yml \u251c\u2500\u2500 projects \u2502 \u251c\u2500\u2500 infrastructure.yml \u2502 \u2514\u2500\u2500 kustomization.yml \u2514\u2500\u2500 vendor \u251c\u2500\u2500 argocd \u251c\u2500\u2500 cert-manager \u251c\u2500\u2500 dex \u251c\u2500\u2500 external-dns \u251c\u2500\u2500 external-secrets \u251c\u2500\u2500 goldpinger \u251c\u2500\u2500 grafana \u251c\u2500\u2500 ingress-nginx \u251c\u2500\u2500 loki \u251c\u2500\u2500 olm \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 tempo \u2514\u2500\u2500 velero The vendor folder contains Kubernetes manifests for all Karavel components. Files in this folder are not supposed to be edited by hand, as any change would be overwritten by future Karavel updates. If you need to customize them follow the Customization guide . The applications directory contains ArgoCD application manifests that are used to register the vendored components with the GitOps engine. These files can be safely edited to customize how ArgoCD manages them. The projects directory contains ArgoCD project manifests. It will only contain one project, infrastructure , which is used to group all installed Karavel applications. Feel free to add your own. Finally, kustomization.yml is a Kustomize stack that references all the vendored components that are needed to bootstrap the cluster, as well as the application manifests, and can be used to install the entire platform in one go. Once the bootstrap components are up and running, ArgoCD will pick the rest of the applications up and take care of deploying the rest of the stack. To bootstrap the cluster, run the following command while connected to it: kustomize build . | kubectl apply -f - Warning Due to the way kubectl applies multiple manifests at once, there may be some race conditions between created resources. If you get some errors along the lines of no matches for kind \"AppProject\" in version \"argoproj.io/v1alpha1\" just rerun the command again. Also notice that some custom resources provided by non-bootstrap components (e.g. ServiceMonitor provided by prometheus ) will fail to install because their component is not part of the bootstrap process. This is fine and can be ignored. Once ArgoCD is up and running it will take care of updating the missing parts. You can check that all bootstrap components are correctly deployed by running kubectl get pods --all-namespaces .","title":"Bootstrap"},{"location":"quickstart/#git-push","text":"Now that the repository is ready we can commit and push it to the upstream remote. git add --all . && git commit -m \"Bootstrap new cluster\" git push origin main Once pushed ArgoCD will automatically pick it up and start syncing the manifests with the cluster state. New changes can be deployed by simply committing them and pushing to origin . Congratulations, you are now running a full fledged Karavel instance!","title":"Git Push"},{"location":"quickstart/#next-steps","text":"","title":"Next steps"},{"location":"quickstart/#updating-components","text":"Karavel publishes regular updates to its components, either to fix or improve their manifests or to introduce new features and integrations between them. Updates are also released when a new Kubernetes version is published, to keep components compatible (see the Karavel compatibility matrix ). You should really strive to maintain your clusters up to date with the latest Kubernetes release, and Karavel should be updated consequently. To do so, simply change the required component versions and update the parameters if necessary based on the updated documentation, then rerun the karavel render command. This command is idempotent, so it is safe to run multiple times.","title":"Updating components"},{"location":"requirements/","text":"Requirements \u00b6 Karavel is a complex system comprised of a wide selection for components. While the vast majority of them is entirely self-contained, some pieces need external supporting infrastructure in order to function. This infrastructure needs to be provisioned beforehand and configured into Karavel. The documentation for each component lists the exact requirements for each case, but as a general overview the needed infrastructure parts are: a conformant Kubernetes cluster , of course a secure secrets store to store credentials and other passwords, like Hashicorp Vault or AWS Secrets Manager , see components/external-secrets an OIDC compliant SSO service like Google Workspace, Azure AD, Keycloak or any other OIDC service, to authenticate web UIs like ArgoCD and Grafana an S3-compatible object storage server to store data like metrics, logs and traces a supported DNS provider, see components/external-dns The cluster should be able to host the platform component with enough capacity to spare to enable incremental rollouts and autoscaling depending on resource usage, in addition to your own workloads. We recommend to start with at least three nodes with 4 cores and 8 GiB of RAM each, although this can be reduced by using only a few KCP components and scaling down some of the less critical pods.","title":"Requirements"},{"location":"requirements/#requirements","text":"Karavel is a complex system comprised of a wide selection for components. While the vast majority of them is entirely self-contained, some pieces need external supporting infrastructure in order to function. This infrastructure needs to be provisioned beforehand and configured into Karavel. The documentation for each component lists the exact requirements for each case, but as a general overview the needed infrastructure parts are: a conformant Kubernetes cluster , of course a secure secrets store to store credentials and other passwords, like Hashicorp Vault or AWS Secrets Manager , see components/external-secrets an OIDC compliant SSO service like Google Workspace, Azure AD, Keycloak or any other OIDC service, to authenticate web UIs like ArgoCD and Grafana an S3-compatible object storage server to store data like metrics, logs and traces a supported DNS provider, see components/external-dns The cluster should be able to host the platform component with enough capacity to spare to enable incremental rollouts and autoscaling depending on resource usage, in addition to your own workloads. We recommend to start with at least three nodes with 4 cores and 8 GiB of RAM each, although this can be reduced by using only a few KCP components and scaling down some of the less critical pods.","title":"Requirements"},{"location":"versioning/","text":"Versioning \u00b6 The Karavel Container Platform aims at supporting the current version of Kubernetes plus two prior versions. This provides enough time for operators to maintain their Karavel clusters with confidence and clear upgrade paths. Karavel maintains two versioning policies: a per-component policy and a platform-wide policy . Component versions follow Semantic Versioning principles and are always in the major.minor.patch format (e.g. 0.3.1 ). Platform versions follow a Calendar Versioning schema composed as yyyy.minor.patch (e.g. 2021.2.3 ). Component versioning policy \u00b6 Each Karavel component is packaged and versioned individually. Component versions may increase because of changes being made in the component chart, or because the packaged software version has been updated. In the latter case the component version will be incremented as well to reflect the new upstream version. For instance, if cert-manager updates from 1.0.0 to 1.0.1 , the component's patch version will be increased. If cert-manager updates from 1.0.0 to 1.1.0 , this will update the component's minor version. Finally, if cert-manager releases a shiny new 2.0.0 version a new major version of the component will be released as well Sometimes however, the Karavel project will release a new version of a component to reflect changes made to the chart or the manifests it deploys, without changing the upstream software version. These changes will trigger a version bump according to the rules of Semantic Versioning . A backward compatible bugfix will increment the patch version. A backward compatible improvement or new feature will increment the minor version. A backward incompatible or otherwise breaking change will increment the major version. Platform versioning policy \u00b6 Due to the large amount of components provided by Karavel and the possible interactions between them to compose a production-ready Kubernetes platform, the Karavel project regularly picks specific component versions that have been extensively tested and verified to work well together. These curated selections compose the Karavel Container Platform and are published under a single version, much like a Linux distribution would do with its packages and repositories. This gives developers working on components the freedom to iterate and improve their charts without being tied down to the others, and it gives operators the confidence that their clusters will always work at their best, while still being able to update specific components independently to leverage new features. The Karavel team maintains the Karavel Container Platform in sync with the target Kubernetes versions. Karavel supports the current K8s version plus two prior releases. The Karavel project will drop support for a given Karavel Container Platform release when the lowest compatible Kubernetes version becomes unsupported. As a general recommendation, operators should strive to keep Karavel clusters on the latest minor version of both Kubernetes and the Karavel Container Platform. Kubernetes compatibility matrix \u00b6 Karavel Container Platform Version Kubernetes 1.20 Kubernetes 1.21 Kubernetes 1.22 (current) Kubernetes 1.23 (next) unstable 2021.1 (coming December 2021) compatible specific requirements or caveats are present. Consult the release changelog for more information planned but not yet available actively being developed Cloud providers support matrix \u00b6 The Karavel Container Platform is regularly tested on a number of different Kubernetes managed services in addition to plain upstream Kubernetes. Currently, these are the officially supported providers. Kubernetes versions are tested following the table in the previous section. Support for a managed service illustrates the integration status with other parts of the cloud provider, such as their credentials storage for External Secrets , DNS provider , object storage, and so on. Provider Kubernetes Secrets Storage DNS Object Storage Amazon EKS DigitalOcean Kubernetes offering not available Azure AKS Google GKE For a more updated timeline of planned features, please refer to the official project board .","title":"Versioning"},{"location":"versioning/#versioning","text":"The Karavel Container Platform aims at supporting the current version of Kubernetes plus two prior versions. This provides enough time for operators to maintain their Karavel clusters with confidence and clear upgrade paths. Karavel maintains two versioning policies: a per-component policy and a platform-wide policy . Component versions follow Semantic Versioning principles and are always in the major.minor.patch format (e.g. 0.3.1 ). Platform versions follow a Calendar Versioning schema composed as yyyy.minor.patch (e.g. 2021.2.3 ).","title":"Versioning"},{"location":"versioning/#component-versioning-policy","text":"Each Karavel component is packaged and versioned individually. Component versions may increase because of changes being made in the component chart, or because the packaged software version has been updated. In the latter case the component version will be incremented as well to reflect the new upstream version. For instance, if cert-manager updates from 1.0.0 to 1.0.1 , the component's patch version will be increased. If cert-manager updates from 1.0.0 to 1.1.0 , this will update the component's minor version. Finally, if cert-manager releases a shiny new 2.0.0 version a new major version of the component will be released as well Sometimes however, the Karavel project will release a new version of a component to reflect changes made to the chart or the manifests it deploys, without changing the upstream software version. These changes will trigger a version bump according to the rules of Semantic Versioning . A backward compatible bugfix will increment the patch version. A backward compatible improvement or new feature will increment the minor version. A backward incompatible or otherwise breaking change will increment the major version.","title":"Component versioning policy"},{"location":"versioning/#platform-versioning-policy","text":"Due to the large amount of components provided by Karavel and the possible interactions between them to compose a production-ready Kubernetes platform, the Karavel project regularly picks specific component versions that have been extensively tested and verified to work well together. These curated selections compose the Karavel Container Platform and are published under a single version, much like a Linux distribution would do with its packages and repositories. This gives developers working on components the freedom to iterate and improve their charts without being tied down to the others, and it gives operators the confidence that their clusters will always work at their best, while still being able to update specific components independently to leverage new features. The Karavel team maintains the Karavel Container Platform in sync with the target Kubernetes versions. Karavel supports the current K8s version plus two prior releases. The Karavel project will drop support for a given Karavel Container Platform release when the lowest compatible Kubernetes version becomes unsupported. As a general recommendation, operators should strive to keep Karavel clusters on the latest minor version of both Kubernetes and the Karavel Container Platform.","title":"Platform versioning policy"},{"location":"versioning/#kubernetes-compatibility-matrix","text":"Karavel Container Platform Version Kubernetes 1.20 Kubernetes 1.21 Kubernetes 1.22 (current) Kubernetes 1.23 (next) unstable 2021.1 (coming December 2021) compatible specific requirements or caveats are present. Consult the release changelog for more information planned but not yet available actively being developed","title":"Kubernetes compatibility matrix"},{"location":"versioning/#cloud-providers-support-matrix","text":"The Karavel Container Platform is regularly tested on a number of different Kubernetes managed services in addition to plain upstream Kubernetes. Currently, these are the officially supported providers. Kubernetes versions are tested following the table in the previous section. Support for a managed service illustrates the integration status with other parts of the cloud provider, such as their credentials storage for External Secrets , DNS provider , object storage, and so on. Provider Kubernetes Secrets Storage DNS Object Storage Amazon EKS DigitalOcean Kubernetes offering not available Azure AKS Google GKE For a more updated timeline of planned features, please refer to the official project board .","title":"Cloud providers support matrix"},{"location":"components/argocd/","text":"Argo CD Component \u00b6 Documentation Overview \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. License \u00b6 The Argo CD Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/argocd/#argo-cd-component","text":"Documentation","title":"Argo CD Component"},{"location":"components/argocd/#overview","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.","title":"Overview"},{"location":"components/argocd/#license","text":"The Argo CD Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/argocd/1.0/configuration/","text":"Configuration \u00b6 component \"argocd\" { namespace = \"argocd\" # Params default values publicURL = \"\" # required, the full URL to reach the Argo instance, e.g. https://deploy.company.com git = { repo = \"\" # required, the git URL containing the Karavel code, e.g. git@github.com:example-company/cloud-infrastructure.git } adminGroup = \"\" # optional, the OIDC group that should be given full admin powers credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key type = \"ssh\" # optional, values: ssh or password url = \"\" # optional, overrides git.repo in the credential secret template = false # optional, configures the secret as a credentials template, allowing to pass a git namespace as the url above when true (e.g. git@github.com:my-org) } secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } # override this section only if you are using # a different Dex instance than the default one dex = { name = \"dex\" namespace = \"dex\" } # override this section only if you are NOT using the Dex component oidc = { config = { name = \"SSO\" issuer = \"\" # required clientID = \"\" # required clientSecret = \"$oidc.clientSecret\" # see https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets cliClientID = \"\" # optional requestedScopes = [ \"openid\", \"profile\", \"email\", \"groups\", ] } } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing ArgoCD. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/argocd/1.0/configuration/#configuration","text":"component \"argocd\" { namespace = \"argocd\" # Params default values publicURL = \"\" # required, the full URL to reach the Argo instance, e.g. https://deploy.company.com git = { repo = \"\" # required, the git URL containing the Karavel code, e.g. git@github.com:example-company/cloud-infrastructure.git } adminGroup = \"\" # optional, the OIDC group that should be given full admin powers credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key type = \"ssh\" # optional, values: ssh or password url = \"\" # optional, overrides git.repo in the credential secret template = false # optional, configures the secret as a credentials template, allowing to pass a git namespace as the url above when true (e.g. git@github.com:my-org) } secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } # override this section only if you are using # a different Dex instance than the default one dex = { name = \"dex\" namespace = \"dex\" } # override this section only if you are NOT using the Dex component oidc = { config = { name = \"SSO\" issuer = \"\" # required clientID = \"\" # required clientSecret = \"$oidc.clientSecret\" # see https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets cliClientID = \"\" # optional requestedScopes = [ \"openid\", \"profile\", \"email\", \"groups\", ] } } }","title":"Configuration"},{"location":"components/argocd/1.0/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing ArgoCD. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/argocd/1.1/configuration/","text":"Configuration \u00b6 component \"argocd\" { namespace = \"argocd\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Argo instance, e.g. https://deploy.company.com git = { repo = \"\" # required, the git URL containing the Karavel code, e.g. git@github.com:example-company/cloud-infrastructure.git } adminGroup = \"\" # optional, the OIDC group that should be given full admin powers credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key type = \"ssh\" # optional, values: ssh or password url = \"\" # optional, overrides git.repo in the credential secret template = false # optional, configures the secret as a credentials template, allowing to pass a git namespace as the url above when true (e.g. git@github.com:my-org) } secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } # override this section only if you are using # a different Dex instance than the default one dex = { name = \"dex\" namespace = \"dex\" } # override this section only if you are NOT using the Dex component oidc = { config = { name = \"SSO\" issuer = \"\" # required clientID = \"\" # required clientSecret = \"$oidc.clientSecret\" # see https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets cliClientID = \"\" # optional requestedScopes = [ \"openid\", \"profile\", \"email\", \"groups\", ] } } tracing = { enabled = false # optional, automatically enabled by the Tempo component endpoint = \"tempo.monitoring.svc.cluster.local:4317\" # optional, the OTLP address to use. } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing ArgoCD. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/argocd/1.1/configuration/#configuration","text":"component \"argocd\" { namespace = \"argocd\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Argo instance, e.g. https://deploy.company.com git = { repo = \"\" # required, the git URL containing the Karavel code, e.g. git@github.com:example-company/cloud-infrastructure.git } adminGroup = \"\" # optional, the OIDC group that should be given full admin powers credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key type = \"ssh\" # optional, values: ssh or password url = \"\" # optional, overrides git.repo in the credential secret template = false # optional, configures the secret as a credentials template, allowing to pass a git namespace as the url above when true (e.g. git@github.com:my-org) } secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } # override this section only if you are using # a different Dex instance than the default one dex = { name = \"dex\" namespace = \"dex\" } # override this section only if you are NOT using the Dex component oidc = { config = { name = \"SSO\" issuer = \"\" # required clientID = \"\" # required clientSecret = \"$oidc.clientSecret\" # see https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets cliClientID = \"\" # optional requestedScopes = [ \"openid\", \"profile\", \"email\", \"groups\", ] } } tracing = { enabled = false # optional, automatically enabled by the Tempo component endpoint = \"tempo.monitoring.svc.cluster.local:4317\" # optional, the OTLP address to use. } }","title":"Configuration"},{"location":"components/argocd/1.1/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing ArgoCD. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/cert-manager/","text":"cert-manager Component \u00b6 Documentation Overview \u00b6 Automates TLS certificates provisioning from various sources, including ACME (Let's Encrypt), HashiCorp Vault, Venafi, self signed and private certificate authorities License \u00b6 The cert-manager Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/cert-manager/#cert-manager-component","text":"Documentation","title":"cert-manager Component"},{"location":"components/cert-manager/#overview","text":"Automates TLS certificates provisioning from various sources, including ACME (Let's Encrypt), HashiCorp Vault, Venafi, self signed and private certificate authorities","title":"Overview"},{"location":"components/cert-manager/#license","text":"The cert-manager Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/cert-manager/1.0/configuration/","text":"Configuration \u00b6 component \"cert-manager\" { namespace = \"cert-manager\" # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } } Route53 \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge. CloudFlare \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Configuration"},{"location":"components/cert-manager/1.0/configuration/#configuration","text":"component \"cert-manager\" { namespace = \"cert-manager\" # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } }","title":"Configuration"},{"location":"components/cert-manager/1.0/configuration/#route53","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Route53"},{"location":"components/cert-manager/1.0/configuration/#cloudflare","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"CloudFlare"},{"location":"components/cert-manager/1.1/configuration/","text":"Configuration \u00b6 component \"cert-manager\" { namespace = \"cert-manager\" # optional # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } } Route53 \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge. CloudFlare \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Configuration"},{"location":"components/cert-manager/1.1/configuration/#configuration","text":"component \"cert-manager\" { namespace = \"cert-manager\" # optional # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } }","title":"Configuration"},{"location":"components/cert-manager/1.1/configuration/#route53","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Route53"},{"location":"components/cert-manager/1.1/configuration/#cloudflare","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"CloudFlare"},{"location":"components/cert-manager/1.2/configuration/","text":"Configuration \u00b6 component \"cert-manager\" { namespace = \"cert-manager\" # optional # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } } Route53 \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge. CloudFlare \u00b6 See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Configuration"},{"location":"components/cert-manager/1.2/configuration/#configuration","text":"component \"cert-manager\" { namespace = \"cert-manager\" # optional # Params default values letsencrypt = { email = \"\" # required, the email to associate with the Let's Encrypt account route53 = { enable = false # enable Route53 DNS01 challenge region = \"eu-west-1\" # AWS region of the Route53 zone domains = [] # optional, domains that match this list will use the DNS01 challenge zoneId = \"\" # optional, the Route53 zone ID to match eksRole = \"\" # When running on EKS, the IAM role cert-manager will use to invoke the Route53 API iamRole = \"\" # When running on EC2, the IAM role cert-manager will use to invoke the Route53 API } cloudflare = { enable = false # enable CloudFlare DNS01 challenge email = \"\" # required, CloudFlare email associated with the account domains = [] # optional, domains that match this list will use the DNS01 challenge # External Secrets configuration for pulling the CloudFlare API token from the storage service secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the token if the secret is structured (e.g. a JSON document) } } } }","title":"Configuration"},{"location":"components/cert-manager/1.2/configuration/#route53","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"Route53"},{"location":"components/cert-manager/1.2/configuration/#cloudflare","text":"See cert-manager docs for more information on the permissions required to perform the DNS01 challenge.","title":"CloudFlare"},{"location":"components/dex/","text":"Dex Component \u00b6 Documentation Overview \u00b6 Dex is an identity service that uses OpenID Connect to drive authentication for other apps. Dex acts as a portal to other identity providers through \u201cconnectors.\u201d This lets Dex defer authentication to LDAP servers, SAML providers, or established identity providers like GitHub, Google, and Active Directory. Clients write their authentication logic once to talk to Dex, then Dex handles the protocols for a given backend. License \u00b6 The Dex Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/dex/#dex-component","text":"Documentation","title":"Dex Component"},{"location":"components/dex/#overview","text":"Dex is an identity service that uses OpenID Connect to drive authentication for other apps. Dex acts as a portal to other identity providers through \u201cconnectors.\u201d This lets Dex defer authentication to LDAP servers, SAML providers, or established identity providers like GitHub, Google, and Active Directory. Clients write their authentication logic once to talk to Dex, then Dex handles the protocols for a given backend.","title":"Overview"},{"location":"components/dex/#license","text":"The Dex Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/dex/1.0/configuration/","text":"Configuration \u00b6 component \"dex\" { namespace = \"dex\" # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/dex/1.0/configuration/#configuration","text":"component \"dex\" { namespace = \"dex\" # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } }","title":"Configuration"},{"location":"components/dex/1.0/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/dex/1.1/configuration/","text":"Configuration \u00b6 component \"dex\" { namespace = \"dex\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/dex/1.1/configuration/#configuration","text":"component \"dex\" { namespace = \"dex\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } }","title":"Configuration"},{"location":"components/dex/1.1/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/dex/1.2/configuration/","text":"Configuration \u00b6 component \"dex\" { namespace = \"dex\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/dex/1.2/configuration/#configuration","text":"component \"dex\" { namespace = \"dex\" # optional # Params default values publicURL = \"\" # required, the full URL to reach the Dex instance, e.g. https://oauth.company.com connectors = [ # Add your connectors here # ... # See https://dexidp.io/docs/connectors/ for available choices and their configuration schema # Example entry { type = \"github\" # required, specify \"github\" for GitHub id = \"github\" # required, Connector ID name = \"GitHub\" # required, Connector human readable name config = { # ... config fields as specified in https://dexidp.io/docs/connectors/github/ } ] secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } }","title":"Configuration"},{"location":"components/dex/1.2/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Dex. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/external-dns/","text":"External DNS Component \u00b6 Documentation Overview \u00b6 ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. In a dynamic and elastic environment like Kubernetes, services come and go in the blink of an eye. Developers may want deploy a new service to the public and need a DNS record to route traffic into the cluster. Having to ask an administrator to manually configure the DNS provider can be tedious and a waste of time, especially in large organizations. External DNS bridges the gap between Kubernetes network objects, like Service and Ingress definitions, and configure the upstream DNS provider accordingly, taking care of creating, updating or deleting records as needed. Supported DNS provider \u00b6 Multiple DNS providers are supported. At the moment the following services can be configured: AWS Route53 Cloudflare Support for additional services can be added based on the controller's capabilities. License \u00b6 The External DNS Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/external-dns/#external-dns-component","text":"Documentation","title":"External DNS Component"},{"location":"components/external-dns/#overview","text":"ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. In a dynamic and elastic environment like Kubernetes, services come and go in the blink of an eye. Developers may want deploy a new service to the public and need a DNS record to route traffic into the cluster. Having to ask an administrator to manually configure the DNS provider can be tedious and a waste of time, especially in large organizations. External DNS bridges the gap between Kubernetes network objects, like Service and Ingress definitions, and configure the upstream DNS provider accordingly, taking care of creating, updating or deleting records as needed.","title":"Overview"},{"location":"components/external-dns/#supported-dns-provider","text":"Multiple DNS providers are supported. At the moment the following services can be configured: AWS Route53 Cloudflare Support for additional services can be added based on the controller's capabilities.","title":"Supported DNS provider"},{"location":"components/external-dns/#license","text":"The External DNS Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/external-dns/1.0/configuration/","text":"Configuration \u00b6 component \"external-dns\" { version = \"0.2.0\" namespace = \"external-dns\" # Params default values # when configured with a base domain, external-dns will ignore requests that are not children domains domainFilter = \"\" # Upstream DNS provider to configure # required, must be one of 'cloudflare', 'route53', 'pdns' provider = \"\" cloudflare = { # Enable or disable the Cloudflare Proxy on managed records. Can be overridden on a per-object basis proxied = false # Restrict to domains in a specific Cloudflare Zone. Optional zoneId = \"\" # ExternalSecret object reference to a secret holding the Cloudflare API Token secret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } route53 = { # Only look at zone of this type (values can be 'public', 'private' or empty for both) zoneType = \"\" # Restrict to domains in a specific Route53 Zone. Optional zoneId = \"\" # Configure when deployed on EKS or other platforms with IAM Roles for Service Accounts eksRole = \"\" # Configure when deployed on AWS with KIAM iamRole = \"\" } pdns = { apiUrl = \"\" # ExternalSecret object reference to a secret holding the PowerDNS API key apiKeySecret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } } Route53 \u00b6 To use Route53, a valid IAM role must be created with the following policies: For the hosted zone only: \u00b6 route53:ChangeResourceRecordSets For everything ( \"*\" ): \u00b6 route53:ListHostedZones route53:ListResourceRecordSets","title":"Configuration"},{"location":"components/external-dns/1.0/configuration/#configuration","text":"component \"external-dns\" { version = \"0.2.0\" namespace = \"external-dns\" # Params default values # when configured with a base domain, external-dns will ignore requests that are not children domains domainFilter = \"\" # Upstream DNS provider to configure # required, must be one of 'cloudflare', 'route53', 'pdns' provider = \"\" cloudflare = { # Enable or disable the Cloudflare Proxy on managed records. Can be overridden on a per-object basis proxied = false # Restrict to domains in a specific Cloudflare Zone. Optional zoneId = \"\" # ExternalSecret object reference to a secret holding the Cloudflare API Token secret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } route53 = { # Only look at zone of this type (values can be 'public', 'private' or empty for both) zoneType = \"\" # Restrict to domains in a specific Route53 Zone. Optional zoneId = \"\" # Configure when deployed on EKS or other platforms with IAM Roles for Service Accounts eksRole = \"\" # Configure when deployed on AWS with KIAM iamRole = \"\" } pdns = { apiUrl = \"\" # ExternalSecret object reference to a secret holding the PowerDNS API key apiKeySecret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } }","title":"Configuration"},{"location":"components/external-dns/1.0/configuration/#route53","text":"To use Route53, a valid IAM role must be created with the following policies:","title":"Route53"},{"location":"components/external-dns/1.0/configuration/#for-the-hosted-zone-only","text":"route53:ChangeResourceRecordSets","title":"For the hosted zone only:"},{"location":"components/external-dns/1.0/configuration/#for-everything","text":"route53:ListHostedZones route53:ListResourceRecordSets","title":"For everything (\"*\"):"},{"location":"components/external-dns/1.1/configuration/","text":"Configuration \u00b6 component \"external-dns\" { version = \"0.2.0\" namespace = \"external-dns\" # optional # Params default values # when configured with a base domain, external-dns will ignore requests that are not children domains domainFilter = \"\" # Upstream DNS provider to configure # required, must be one of 'cloudflare', 'route53', 'pdns' provider = \"\" cloudflare = { # Enable or disable the Cloudflare Proxy on managed records. Can be overridden on a per-object basis proxied = false # Restrict to domains in a specific Cloudflare Zone. Optional zoneId = \"\" # ExternalSecret object reference to a secret holding the Cloudflare API Token secret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } route53 = { # Only look at zone of this type (values can be 'public', 'private' or empty for both) zoneType = \"\" # Restrict to domains in a specific Route53 Zone. Optional zoneId = \"\" # Configure when deployed on EKS or other platforms with IAM Roles for Service Accounts eksRole = \"\" # Configure when deployed on AWS with KIAM iamRole = \"\" } pdns = { apiUrl = \"\" # ExternalSecret object reference to a secret holding the PowerDNS API key apiKeySecret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } } Route53 \u00b6 To use Route53, a valid IAM role must be created with the following policies: For the hosted zone only: \u00b6 route53:ChangeResourceRecordSets For everything ( \"*\" ): \u00b6 route53:ListHostedZones route53:ListResourceRecordSets","title":"Configuration"},{"location":"components/external-dns/1.1/configuration/#configuration","text":"component \"external-dns\" { version = \"0.2.0\" namespace = \"external-dns\" # optional # Params default values # when configured with a base domain, external-dns will ignore requests that are not children domains domainFilter = \"\" # Upstream DNS provider to configure # required, must be one of 'cloudflare', 'route53', 'pdns' provider = \"\" cloudflare = { # Enable or disable the Cloudflare Proxy on managed records. Can be overridden on a per-object basis proxied = false # Restrict to domains in a specific Cloudflare Zone. Optional zoneId = \"\" # ExternalSecret object reference to a secret holding the Cloudflare API Token secret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } route53 = { # Only look at zone of this type (values can be 'public', 'private' or empty for both) zoneType = \"\" # Restrict to domains in a specific Route53 Zone. Optional zoneId = \"\" # Configure when deployed on EKS or other platforms with IAM Roles for Service Accounts eksRole = \"\" # Configure when deployed on AWS with KIAM iamRole = \"\" } pdns = { apiUrl = \"\" # ExternalSecret object reference to a secret holding the PowerDNS API key apiKeySecret = { store = { name = \"default\" kind = \"ClusterSecretStore\" } # Backend-specific key for the target secret key = \"\" # Optional nested property inside the upstream secret property = \"\" } } }","title":"Configuration"},{"location":"components/external-dns/1.1/configuration/#route53","text":"To use Route53, a valid IAM role must be created with the following policies:","title":"Route53"},{"location":"components/external-dns/1.1/configuration/#for-the-hosted-zone-only","text":"route53:ChangeResourceRecordSets","title":"For the hosted zone only:"},{"location":"components/external-dns/1.1/configuration/#for-everything","text":"route53:ListHostedZones route53:ListResourceRecordSets","title":"For everything (\"*\"):"},{"location":"components/external-secrets/","text":"External Secrets Component \u00b6 Documentation Overview \u00b6 External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault and many more.\\ License \u00b6 The External Secrets Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/external-secrets/#external-secrets-component","text":"Documentation","title":"External Secrets Component"},{"location":"components/external-secrets/#overview","text":"External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault and many more.\\","title":"Overview"},{"location":"components/external-secrets/#license","text":"The External Secrets Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/external-secrets/1.0/configuration/","text":"Configuration \u00b6 component \"external-secrets\" { namespace = \"external-secrets\" # Params default values defaultProvider = \"\" # required, what secret store to use, available: \"aws\", \"vault\" # required when using AWS Secret Manager aws = { enable = false # required, set to true if using AWS region = \"eu-west-1\" # required, AWS region, defaults to \"eu-west-1\" eksRole = \"\" # optional, EKS role ARN iamRole = \"\" # optional, IAM role ARN } # required when using Hashicorp Vault vault = { enable = false # required, set to true when using Vault address = \"https://vault.vault.svc.cluster.local:8200\" # required, URL to Vault instance namespace = \"\" # required, namespace of Vault instance # optional, override authentication method (only Kubernetes SATs are supported) auth = { kubernetes = { mountPath = \"kubernetes\" role = \"karavel\" } } # optional, override secret engine and path engine = { version = \"v2\" path = \"secret\" } } } AWS Secret Manager \u00b6 To use AWS Secret Manager, you will need to create a valid IAM role with the following read-only policy: secretsmanager:GetResourcePolicy secretsmanager:GetSecretValue secretsmanager:DescribeSecret secretsmanager:ListSecretVersionIds","title":"Configuration"},{"location":"components/external-secrets/1.0/configuration/#configuration","text":"component \"external-secrets\" { namespace = \"external-secrets\" # Params default values defaultProvider = \"\" # required, what secret store to use, available: \"aws\", \"vault\" # required when using AWS Secret Manager aws = { enable = false # required, set to true if using AWS region = \"eu-west-1\" # required, AWS region, defaults to \"eu-west-1\" eksRole = \"\" # optional, EKS role ARN iamRole = \"\" # optional, IAM role ARN } # required when using Hashicorp Vault vault = { enable = false # required, set to true when using Vault address = \"https://vault.vault.svc.cluster.local:8200\" # required, URL to Vault instance namespace = \"\" # required, namespace of Vault instance # optional, override authentication method (only Kubernetes SATs are supported) auth = { kubernetes = { mountPath = \"kubernetes\" role = \"karavel\" } } # optional, override secret engine and path engine = { version = \"v2\" path = \"secret\" } } }","title":"Configuration"},{"location":"components/external-secrets/1.0/configuration/#aws-secret-manager","text":"To use AWS Secret Manager, you will need to create a valid IAM role with the following read-only policy: secretsmanager:GetResourcePolicy secretsmanager:GetSecretValue secretsmanager:DescribeSecret secretsmanager:ListSecretVersionIds","title":"AWS Secret Manager"},{"location":"components/external-secrets/1.1/configuration/","text":"Configuration \u00b6 component \"external-secrets\" { namespace = \"external-secrets\" # optional # Params default values defaultProvider = \"\" # required, what secret store to use, available: \"aws\", \"vault\" # required when using AWS Secret Manager aws = { enable = false # required, set to true if using AWS region = \"eu-west-1\" # required, AWS region, defaults to \"eu-west-1\" eksRole = \"\" # optional, EKS role ARN iamRole = \"\" # optional, IAM role ARN } # required when using Hashicorp Vault vault = { enable = false # required, set to true when using Vault address = \"https://vault.vault.svc.cluster.local:8200\" # required, URL to Vault instance namespace = \"\" # required, namespace of Vault instance # optional, override authentication method (only Kubernetes SATs are supported) auth = { kubernetes = { mountPath = \"kubernetes\" role = \"karavel\" } } # optional, override secret engine and path engine = { version = \"v2\" path = \"secret\" } } } AWS Secret Manager \u00b6 To use AWS Secret Manager, you will need to create a valid IAM role with the following read-only policy: secretsmanager:GetResourcePolicy secretsmanager:GetSecretValue secretsmanager:DescribeSecret secretsmanager:ListSecretVersionIds","title":"Configuration"},{"location":"components/external-secrets/1.1/configuration/#configuration","text":"component \"external-secrets\" { namespace = \"external-secrets\" # optional # Params default values defaultProvider = \"\" # required, what secret store to use, available: \"aws\", \"vault\" # required when using AWS Secret Manager aws = { enable = false # required, set to true if using AWS region = \"eu-west-1\" # required, AWS region, defaults to \"eu-west-1\" eksRole = \"\" # optional, EKS role ARN iamRole = \"\" # optional, IAM role ARN } # required when using Hashicorp Vault vault = { enable = false # required, set to true when using Vault address = \"https://vault.vault.svc.cluster.local:8200\" # required, URL to Vault instance namespace = \"\" # required, namespace of Vault instance # optional, override authentication method (only Kubernetes SATs are supported) auth = { kubernetes = { mountPath = \"kubernetes\" role = \"karavel\" } } # optional, override secret engine and path engine = { version = \"v2\" path = \"secret\" } } }","title":"Configuration"},{"location":"components/external-secrets/1.1/configuration/#aws-secret-manager","text":"To use AWS Secret Manager, you will need to create a valid IAM role with the following read-only policy: secretsmanager:GetResourcePolicy secretsmanager:GetSecretValue secretsmanager:DescribeSecret secretsmanager:ListSecretVersionIds","title":"AWS Secret Manager"},{"location":"components/goldpinger/","text":"Goldpinger Component \u00b6 Documentation Overview \u00b6 Goldpinger is a debugging tool for Kubernetes which tests and displays connectivity between nodes in the cluster License \u00b6 The Goldpinger Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/goldpinger/#goldpinger-component","text":"Documentation","title":"Goldpinger Component"},{"location":"components/goldpinger/#overview","text":"Goldpinger is a debugging tool for Kubernetes which tests and displays connectivity between nodes in the cluster","title":"Overview"},{"location":"components/goldpinger/#license","text":"The Goldpinger Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/goldpinger/1.0/configuration/","text":"Configuration \u00b6 component \"goldpinger\" { namespace = \"goldpinger\" }","title":"Configuration"},{"location":"components/goldpinger/1.0/configuration/#configuration","text":"component \"goldpinger\" { namespace = \"goldpinger\" }","title":"Configuration"},{"location":"components/goldpinger/1.1/configuration/","text":"Configuration \u00b6 component \"goldpinger\" { namespace = \"goldpinger\" # optional }","title":"Configuration"},{"location":"components/goldpinger/1.1/configuration/#configuration","text":"component \"goldpinger\" { namespace = \"goldpinger\" # optional }","title":"Configuration"},{"location":"components/grafana/","text":"Grafana Component \u00b6 Documentation Overview \u00b6 Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored License \u00b6 The Grafana Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/grafana/#grafana-component","text":"Documentation","title":"Grafana Component"},{"location":"components/grafana/#overview","text":"Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored","title":"Overview"},{"location":"components/grafana/#license","text":"The Grafana Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/grafana/1.0.0/configuration/","text":"Configuration \u00b6 component \"grafana\" { namespace = \"grafana\" # Params default values publicURL = \"\" # required, the full URL to reach the Grafana instance, e.g. https://dashboard.company.com secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } dex = { issuer = \"\" # required, Dex OIDC issuer URL # override this section only if you are using # a different Dex instance than the default one name = \"dex\" namespace = \"dex\" } } Secrets \u00b6 Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Grafana. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Configuration"},{"location":"components/grafana/1.0.0/configuration/#configuration","text":"component \"grafana\" { namespace = \"grafana\" # Params default values publicURL = \"\" # required, the full URL to reach the Grafana instance, e.g. https://dashboard.company.com secret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key } dex = { issuer = \"\" # required, Dex OIDC issuer URL # override this section only if you are using # a different Dex instance than the default one name = \"dex\" namespace = \"dex\" } }","title":"Configuration"},{"location":"components/grafana/1.0.0/configuration/#secrets","text":"Secrets referenced by the configuration must be provisioned in the backend service of your choice before installing Grafana. Unless you changed the configuration of the External Secret component or manually provisioned a custom SecretStore resource, you don't need to override the store section of each secret reference.","title":"Secrets"},{"location":"components/ingress-nginx/","text":"NGINX Ingress Controller Component \u00b6 Documentation Overview \u00b6 ingress-nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer. License \u00b6 The NGINX Ingress Controller Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/ingress-nginx/#nginx-ingress-controller-component","text":"Documentation","title":"NGINX Ingress Controller Component"},{"location":"components/ingress-nginx/#overview","text":"ingress-nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.","title":"Overview"},{"location":"components/ingress-nginx/#license","text":"The NGINX Ingress Controller Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/ingress-nginx/1.0/configuration/","text":"Configuration \u00b6 component \"ingress-nginx\" { namespace = \"ingress-nginx\" # Params default values defaultIngress = true # optional, set to false if you want another ingress as default }","title":"Configuration"},{"location":"components/ingress-nginx/1.0/configuration/#configuration","text":"component \"ingress-nginx\" { namespace = \"ingress-nginx\" # Params default values defaultIngress = true # optional, set to false if you want another ingress as default }","title":"Configuration"},{"location":"components/ingress-nginx/1.1/configuration/","text":"Configuration \u00b6 component \"ingress-nginx\" { namespace = \"ingress-nginx\" # optional # Params default values defaultIngress = true # optional, set to false if you want another ingress as default }","title":"Configuration"},{"location":"components/ingress-nginx/1.1/configuration/#configuration","text":"component \"ingress-nginx\" { namespace = \"ingress-nginx\" # optional # Params default values defaultIngress = true # optional, set to false if you want another ingress as default }","title":"Configuration"},{"location":"components/loki/","text":"Loki Component \u00b6 Documentation Overview \u00b6 Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus License \u00b6 The Loki Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/loki/#loki-component","text":"Documentation","title":"Loki Component"},{"location":"components/loki/#overview","text":"Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus","title":"Overview"},{"location":"components/loki/#license","text":"The Loki Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/loki/1.0/configuration/","text":"Configuration \u00b6 component \"loki\" { namespace = \"loki\" # Params default values store = \"filesystem\" # required, storage provider for loki, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/loki/1.0/configuration/#configuration","text":"component \"loki\" { namespace = \"loki\" # Params default values store = \"filesystem\" # required, storage provider for loki, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/loki/1.1/configuration/","text":"Configuration \u00b6 component \"loki\" { namespace = \"loki\" # optional # Params default values store = \"filesystem\" # required, storage provider for loki, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/loki/1.1/configuration/#configuration","text":"component \"loki\" { namespace = \"loki\" # optional # Params default values store = \"filesystem\" # required, storage provider for loki, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/operator-lifecycle-manager/","text":"Operator Lifecycle Manager Component \u00b6 Documentation Overview \u00b6 The Operator Lifecycle Manager (OLM) extends Kubernetes to provide a declarative way to install, manage, and upgrade Operators on a cluster License \u00b6 The Operator Lifecycle Manager Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/operator-lifecycle-manager/#operator-lifecycle-manager-component","text":"Documentation","title":"Operator Lifecycle Manager Component"},{"location":"components/operator-lifecycle-manager/#overview","text":"The Operator Lifecycle Manager (OLM) extends Kubernetes to provide a declarative way to install, manage, and upgrade Operators on a cluster","title":"Overview"},{"location":"components/operator-lifecycle-manager/#license","text":"The Operator Lifecycle Manager Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/operator-lifecycle-manager/1.0/configuration/","text":"Configuration \u00b6 component \"olm\" { namespace = \"olm\" }","title":"Configuration"},{"location":"components/operator-lifecycle-manager/1.0/configuration/#configuration","text":"component \"olm\" { namespace = \"olm\" }","title":"Configuration"},{"location":"components/operator-lifecycle-manager/1.1/configuration/","text":"Configuration \u00b6 component \"olm\" { namespace = \"olm\" }","title":"Configuration"},{"location":"components/operator-lifecycle-manager/1.1/configuration/#configuration","text":"component \"olm\" { namespace = \"olm\" }","title":"Configuration"},{"location":"components/prometheus/","text":"Prometheus Component \u00b6 Documentation Overview \u00b6 Prometheus is a systems and service monitoring system that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed License \u00b6 The Prometheus Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/prometheus/#prometheus-component","text":"Documentation","title":"Prometheus Component"},{"location":"components/prometheus/#overview","text":"Prometheus is a systems and service monitoring system that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed","title":"Overview"},{"location":"components/prometheus/#license","text":"The Prometheus Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/prometheus/1.0/configuration/","text":"Configuration \u00b6 component \"prometheus\" { namespace = \"prometheus\" store = \"filesystem\" # required, storage provider for thanos, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/prometheus/1.0/configuration/#configuration","text":"component \"prometheus\" { namespace = \"prometheus\" store = \"filesystem\" # required, storage provider for thanos, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # required, S3 endpoint encrypted = true # required, enable to use server-side encryption insecure = false # required, enable to skip server certificate validation pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/tempo/","text":"Tempo Component \u00b6 Documentation Overview \u00b6 Tempo is a high volume, minimal dependency distributed tracing backend. License \u00b6 The Tempo Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/tempo/#tempo-component","text":"Documentation","title":"Tempo Component"},{"location":"components/tempo/#overview","text":"Tempo is a high volume, minimal dependency distributed tracing backend.","title":"Overview"},{"location":"components/tempo/#license","text":"The Tempo Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/tempo/1.0/configuration/","text":"Configuration \u00b6 component \"tempo\" { namespace = \"tempo\" # Params default values store = \"filesystem\" # required, storage provider for tempo, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # optional, S3 endpoint if different from the default AWS endpoint encrypted = true # optional, enable to use server-side encryption insecure = false # optional, enable to skip server certificate validation pathStyle = false # optional, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/tempo/1.0/configuration/#configuration","text":"component \"tempo\" { namespace = \"tempo\" # Params default values store = \"filesystem\" # required, storage provider for tempo, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # optional, S3 endpoint if different from the default AWS endpoint encrypted = true # optional, enable to use server-side encryption insecure = false # optional, enable to skip server certificate validation pathStyle = false # optional, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/tempo/1.1/configuration/","text":"Configuration \u00b6 component \"tempo\" { namespace = \"tempo\" # optional # Params default values store = \"filesystem\" # required, storage provider for tempo, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # optional, S3 endpoint if different from the default AWS endpoint encrypted = true # optional, enable to use server-side encryption insecure = false # optional, enable to skip server certificate validation pathStyle = false # optional, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/tempo/1.1/configuration/#configuration","text":"component \"tempo\" { namespace = \"tempo\" # optional # Params default values store = \"filesystem\" # required, storage provider for tempo, available options: \"filesystem\", \"s3\" # if using s3 for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region bucket = \"\" # required, bucket name endpoint = \"\" # optional, S3 endpoint if different from the default AWS endpoint encrypted = true # optional, enable to use server-side encryption insecure = false # optional, enable to skip server certificate validation pathStyle = false # optional, enable to force path-style requests (eg. minio and similar providers) eksRole = \"\" # optional, EKS role iamRole = \"\" # optional, IAM role } # override if grafana is running in a different namespace grafana = { namespace: \"\" # optional } }","title":"Configuration"},{"location":"components/velero/","text":"Velero Component \u00b6 Documentation Overview \u00b6 Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes License \u00b6 The Velero Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"Overview"},{"location":"components/velero/#velero-component","text":"Documentation","title":"Velero Component"},{"location":"components/velero/#overview","text":"Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes","title":"Overview"},{"location":"components/velero/#license","text":"The Velero Component is licensed under the Apache 2.0 license . The Karavel Container Platform is licensed under the Apache 2.0 license .","title":"License"},{"location":"components/velero/1.0/configuration/","text":"Configuration \u00b6 component \"velero\" { namespace = \"velero\" # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] } Bucket \u00b6 When using AWS as the storage provider you will need to create a S3 bucket. IAM Role \u00b6 When assigning an IAM role for Velero on EKS, you should create a role with the following policies: To backup bucket ARN, for any object ( \"*\" ) \u00b6 s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts To any resource ( \"*\" ) \u00b6 ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"Configuration"},{"location":"components/velero/1.0/configuration/#configuration","text":"component \"velero\" { namespace = \"velero\" # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] }","title":"Configuration"},{"location":"components/velero/1.0/configuration/#bucket","text":"When using AWS as the storage provider you will need to create a S3 bucket.","title":"Bucket"},{"location":"components/velero/1.0/configuration/#iam-role","text":"When assigning an IAM role for Velero on EKS, you should create a role with the following policies:","title":"IAM Role"},{"location":"components/velero/1.0/configuration/#to-backup-bucket-arn-for-any-object","text":"s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts","title":"To backup bucket ARN, for any object (\"*\")"},{"location":"components/velero/1.0/configuration/#to-any-resource","text":"ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"To any resource (\"*\")"},{"location":"components/velero/1.1/configuration/","text":"Configuration \u00b6 component \"velero\" { namespace = \"velero\" # optional # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] } Bucket \u00b6 When using AWS as the storage provider you will need to create a S3 bucket. IAM Role \u00b6 When assigning an IAM role for Velero on EKS, you should create a role with the following policies: To backup bucket ARN, for any object ( \"*\" ) \u00b6 s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts To any resource ( \"*\" ) \u00b6 ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"Configuration"},{"location":"components/velero/1.1/configuration/#configuration","text":"component \"velero\" { namespace = \"velero\" # optional # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] }","title":"Configuration"},{"location":"components/velero/1.1/configuration/#bucket","text":"When using AWS as the storage provider you will need to create a S3 bucket.","title":"Bucket"},{"location":"components/velero/1.1/configuration/#iam-role","text":"When assigning an IAM role for Velero on EKS, you should create a role with the following policies:","title":"IAM Role"},{"location":"components/velero/1.1/configuration/#to-backup-bucket-arn-for-any-object","text":"s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts","title":"To backup bucket ARN, for any object (\"*\")"},{"location":"components/velero/1.1/configuration/#to-any-resource","text":"ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"To any resource (\"*\")"},{"location":"components/velero/1.2/configuration/","text":"Configuration \u00b6 component \"velero\" { namespace = \"velero\" # optional # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] } Bucket \u00b6 When using AWS as the storage provider you will need to create a S3 bucket. IAM Role \u00b6 When assigning an IAM role for Velero on EKS, you should create a role with the following policies: To backup bucket ARN, for any object ( \"*\" ) \u00b6 s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts To any resource ( \"*\" ) \u00b6 ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"Configuration"},{"location":"components/velero/1.2/configuration/#configuration","text":"component \"velero\" { namespace = \"velero\" # optional # Params default values aws = { eksRole = \"\" # optional, IAM Role ARN for EKS RBAC iamRole = \"\" # optional, IAM Role ARN for deploying } backups = { provider = \"\" # required, storage provider for backups, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested # if using aws for storage, this section is required and must point to an S3 bucket s3 = { region = \"\" # required, bucket region endpoint = \"\" # required, S3 endpoint pathStyle = false # required, enable to force path-style requests (eg. minio and similar providers) insecure = false # required, enable to skip server certificate validation encrypted = true # required, enable to use server-side encryption bucket = \"\" # required, bucket name prefix = \"\" # optional, prefix/directory to upload backups into caCert = \"\" # optional, base64 encoded CA bundle to be used for verifying TLS connections # optional, secret ref to AWS Access key ID (overrides credentialsSecret/credentialsPath) accessKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Access key ID } # optional, secret ref to AWS Secret access key (overrides credentialsSecret/credentialsPath) secretKeySecret = { name = \"\" # required, name of the secret object key = \"\" # required, key containing the Secret access key } } } snapshots = { provider = \"\" # required, storage provider for snapshots, available: \"aws\" # other providers in https://velero.io/docs/v1.5/supported-providers/ might work but haven't been tested region = \"\" # required, aws region } restic = { enable = false # required, set to true to enable Restic podVolumePath = \"/var/lib/kubelet/pods\" # optional, path to mount with pod volumes privileged = false # optional, set to true to set Restic pods as privileged } # optional, use this block if you are storing your credentials in a secure store using external-secret credentialsSecret = { # override this section only if you are not using the default store from the external-secrets component store = { name = \"default\" kind = \"ClusterSecretStore\" } key = \"\" # required, should be the store-specific key to the secret, e.g. the Vault or AWS Secrets Manager key property = \"\" # optional, should be the store-specific property inside the secret containing the cloud config text if the secret is structured (e.g. a JSON document) } # optional, path to a AWS credentials file (like the one generated by `aws configure`) # this overrides credentialsSecret but is overridden by accessKeySecret/secretKeySecret # note that you'll have to manually mount the file at this location credentialsPath = \"\" # optional, list of Velero plugins to include # note that if you want to add plugins while keeping the default AWS one you have to copy it again, # because the list would be overridden plugins = [ { name = \"aws\", image = \"velero/velero-plugin-for-aws\", tag = \"v1.3.0\", } ] }","title":"Configuration"},{"location":"components/velero/1.2/configuration/#bucket","text":"When using AWS as the storage provider you will need to create a S3 bucket.","title":"Bucket"},{"location":"components/velero/1.2/configuration/#iam-role","text":"When assigning an IAM role for Velero on EKS, you should create a role with the following policies:","title":"IAM Role"},{"location":"components/velero/1.2/configuration/#to-backup-bucket-arn-for-any-object","text":"s3:ListBucket s3:PutObject s3:GetObject s3:DeleteObject s3:AbortMultipartUpload s3:ListMultipartUploadParts","title":"To backup bucket ARN, for any object (\"*\")"},{"location":"components/velero/1.2/configuration/#to-any-resource","text":"ec2:DescribeVolumes ec2:DescribeSnapshots ec2:CreateTags ec2:CreateVolume ec2:CreateSnapshot ec2:DeleteSnapshot","title":"To any resource (\"*\")"},{"location":"operator-guides/customizing/","text":"Customizing Karavel manifests \u00b6 If you have used the Karavel CLI tool to generate a new Karavel GitOps repository, you should find yourself with a huge vendor directory. This folder contains the Kubernetes manifests for every component managed by Karavel. These manifests have been carefully configured and crafted to work out-of-the-box, but sometimes special configuration must be applied in order to comply with special scenarios or requirements. Since the vendor directory is managed by the Karavel CLI and changes to its files could be overwritten by future updates, hand-editing them is not recommended. Instead, each folder is provisioned as a Kustomize stack. This allows to edit Karavel manifests by applying a Kustomize overlay that patches the Kubernetes objects. Patching a vendored resource \u00b6 For example, let's say we want to scale the NGINX Ingress Controller from the default 3 pods to 6, for increased resiliency. The deployment manifests resides in vendor/ingress-nginx/deployment-ingress-nginx-controller.yml , and the field we want to edit is spec.replicas . Let's start by adding a directory where our patch will live. In the root of your Karavel directory add a new folder called ingress-nginx , at the same level as the vendor directory. . | -- ingress-nginx | -- vendor ` -- # rest of the files and folders omitted Inside this new directory we'll add our YAML patch. The patch needs to reference the apiVersion , kind , metadata.name and metadata.namespace of the target object, as well as any changes to the rest of the manifest we want to merge. In our case, the new spec.replicas . # ingress-nginx/deployment-patch.yml apiVersion : apps/v1 kind : Deployment metadata : name : ingress-nginx-controller namespace : ingress-nginx spec : replicas : 6 # increase replicas from 3 to 6 Now we need to reference this patch in a new Kustomize stack. Let's add a new file called kustomization.yml to the folder. # ingress-nginx/kustomization.yml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - ../vendor/ingress-nginx patchesStrategicMerge : - deployment-patch.yml As you can see the Kustomize stack references the vendored ingress-nginx directory as a base resource, adding our new file as a patch. This imports all the base Karavel manifests, leaving them untouched except for the controller deployment. You can check that the patch is being applied correctly by running the following commands: $ kustomize build vendor/ingress-nginx | grep replicas replicas: 3 $ kustomize build ingress-nginx | grep replicas replicas: 6 The directory structure should be something like this: . \u251c\u2500\u2500 ingress-nginx \u2502 \u251c\u2500\u2500 deployment-patch.yml \u2502 \u2514\u2500\u2500 kustomization.yml \u251c\u2500\u2500 vendor \u2502 \u251c\u2500\u2500 ingress-nginx \u2502 \u2514\u2500\u2500 # rest of the Karavel components omitted \u2514\u2500\u2500 # rest of the files and folders omitted Updating the ArgoCD application \u00b6 Adding a patch is only half of the work. Now we need to tell ArgoCD to look it up and apply it when deploying the manifests. By default the ArgoCD applications definitions point to the vendored manifests. We need to change the path to point to our Kustomize overlay. Here is the Application definition for the NGINX Ingress Controller as generated by the Bootstrap Tool: # applications/ingress-nginx.yml apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ingress-nginx namespace : argocd annotations : argocd.argoproj.io/manifest-generate-paths : . spec : project : infrastructure source : repoURL : git@github.com:example/infra.git path : vendor/ingress-nginx destination : server : https://kubernetes.default.svc namespace : ingress-nginx # other fields omitted for brevity We need to change the spec.source.path field in order to point it to our new folder. We also need to change the annotation for the manifest generation path. This helps ArgoCD detect changes faster by pointing it to the directories this applications is using. Very useful in monorepo situations like a KCP project, where multiple applications are managed with a single Git repository. # applications/ingress-nginx.yml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ingress-nginx namespace: argocd annotations: - argocd.argoproj.io/manifest-generate-paths: . + argocd.argoproj.io/manifest-generate-paths: .;../vendor/ingress-nginx spec: project: infrastructure source: repoURL: git@github.com:example/infra.git - path: vendor/ingress-nginx + path: ingress-nginx destination: server: https://kubernetes.default.svc namespace: ingress-nginx # other fields omitted for brevity All is left to do is committing and pushing our changes to the origin repo. ArgoCD will pick up the new Application definition, apply the Kustomize overlay and deploy the changes, including our patched NGINX deployment. $ git add --all . && git commit -m \"Increase NGINX replicas\" $ git push origin master","title":"Customizing Karavel manifests"},{"location":"operator-guides/customizing/#customizing-karavel-manifests","text":"If you have used the Karavel CLI tool to generate a new Karavel GitOps repository, you should find yourself with a huge vendor directory. This folder contains the Kubernetes manifests for every component managed by Karavel. These manifests have been carefully configured and crafted to work out-of-the-box, but sometimes special configuration must be applied in order to comply with special scenarios or requirements. Since the vendor directory is managed by the Karavel CLI and changes to its files could be overwritten by future updates, hand-editing them is not recommended. Instead, each folder is provisioned as a Kustomize stack. This allows to edit Karavel manifests by applying a Kustomize overlay that patches the Kubernetes objects.","title":"Customizing Karavel manifests"},{"location":"operator-guides/customizing/#patching-a-vendored-resource","text":"For example, let's say we want to scale the NGINX Ingress Controller from the default 3 pods to 6, for increased resiliency. The deployment manifests resides in vendor/ingress-nginx/deployment-ingress-nginx-controller.yml , and the field we want to edit is spec.replicas . Let's start by adding a directory where our patch will live. In the root of your Karavel directory add a new folder called ingress-nginx , at the same level as the vendor directory. . | -- ingress-nginx | -- vendor ` -- # rest of the files and folders omitted Inside this new directory we'll add our YAML patch. The patch needs to reference the apiVersion , kind , metadata.name and metadata.namespace of the target object, as well as any changes to the rest of the manifest we want to merge. In our case, the new spec.replicas . # ingress-nginx/deployment-patch.yml apiVersion : apps/v1 kind : Deployment metadata : name : ingress-nginx-controller namespace : ingress-nginx spec : replicas : 6 # increase replicas from 3 to 6 Now we need to reference this patch in a new Kustomize stack. Let's add a new file called kustomization.yml to the folder. # ingress-nginx/kustomization.yml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - ../vendor/ingress-nginx patchesStrategicMerge : - deployment-patch.yml As you can see the Kustomize stack references the vendored ingress-nginx directory as a base resource, adding our new file as a patch. This imports all the base Karavel manifests, leaving them untouched except for the controller deployment. You can check that the patch is being applied correctly by running the following commands: $ kustomize build vendor/ingress-nginx | grep replicas replicas: 3 $ kustomize build ingress-nginx | grep replicas replicas: 6 The directory structure should be something like this: . \u251c\u2500\u2500 ingress-nginx \u2502 \u251c\u2500\u2500 deployment-patch.yml \u2502 \u2514\u2500\u2500 kustomization.yml \u251c\u2500\u2500 vendor \u2502 \u251c\u2500\u2500 ingress-nginx \u2502 \u2514\u2500\u2500 # rest of the Karavel components omitted \u2514\u2500\u2500 # rest of the files and folders omitted","title":"Patching a vendored resource"},{"location":"operator-guides/customizing/#updating-the-argocd-application","text":"Adding a patch is only half of the work. Now we need to tell ArgoCD to look it up and apply it when deploying the manifests. By default the ArgoCD applications definitions point to the vendored manifests. We need to change the path to point to our Kustomize overlay. Here is the Application definition for the NGINX Ingress Controller as generated by the Bootstrap Tool: # applications/ingress-nginx.yml apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ingress-nginx namespace : argocd annotations : argocd.argoproj.io/manifest-generate-paths : . spec : project : infrastructure source : repoURL : git@github.com:example/infra.git path : vendor/ingress-nginx destination : server : https://kubernetes.default.svc namespace : ingress-nginx # other fields omitted for brevity We need to change the spec.source.path field in order to point it to our new folder. We also need to change the annotation for the manifest generation path. This helps ArgoCD detect changes faster by pointing it to the directories this applications is using. Very useful in monorepo situations like a KCP project, where multiple applications are managed with a single Git repository. # applications/ingress-nginx.yml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ingress-nginx namespace: argocd annotations: - argocd.argoproj.io/manifest-generate-paths: . + argocd.argoproj.io/manifest-generate-paths: .;../vendor/ingress-nginx spec: project: infrastructure source: repoURL: git@github.com:example/infra.git - path: vendor/ingress-nginx + path: ingress-nginx destination: server: https://kubernetes.default.svc namespace: ingress-nginx # other fields omitted for brevity All is left to do is committing and pushing our changes to the origin repo. ArgoCD will pick up the new Application definition, apply the Kustomize overlay and deploy the changes, including our patched NGINX deployment. $ git add --all . && git commit -m \"Increase NGINX replicas\" $ git push origin master","title":"Updating the ArgoCD application"}]}